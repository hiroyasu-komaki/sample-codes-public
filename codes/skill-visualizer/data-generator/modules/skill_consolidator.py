"""
ã‚¹ã‚­ãƒ«ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
åˆ†å‰²ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆã—ã¦1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¾ã¨ã‚ã‚‹
"""

import os
import pandas as pd
from pathlib import Path


class SkillConsolidator:
    """åˆ†å‰²ã•ã‚ŒãŸã‚¹ã‚­ãƒ«æ¨™æº–CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, input_dir='output', output_dir='output2'):
        """
        åˆæœŸåŒ–
        
        Args:
            input_dir (str): å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
            output_dir (str): å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        """
        self.input_dir = input_dir
        self.output_dir = output_dir
        self.consolidated_df = None
        
    def consolidate_csv_files(self):
        """
        å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆ
        
        Returns:
            pd.DataFrame: çµ±åˆã•ã‚ŒãŸDataFrame
        """
        # å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
        if not os.path.exists(self.input_dir):
            raise FileNotFoundError(f"å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.input_dir}")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        csv_files = [f for f in os.listdir(self.input_dir) if f.endswith('.csv')]
        
        if not csv_files:
            raise FileNotFoundError(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.input_dir}")
        
        print(f"ğŸ“ Found {len(csv_files)} CSV files")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¹ãƒˆ
        df_list = []
        
        # å„CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        for csv_file in csv_files:
            file_path = os.path.join(self.input_dir, csv_file)
            
            try:
                # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
                df = pd.read_csv(file_path, encoding='utf-8')
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‚«ãƒ©ãƒ ã‚’è¿½åŠ 
                df['ãƒ•ã‚¡ã‚¤ãƒ«å'] = csv_file
                
                df_list.append(df)
                print(f"   âœ“ Loaded: {csv_file} ({len(df)} rows)")
                
            except Exception as e:
                print(f"   âœ— Error loading {csv_file}: {e}")
                continue
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’çµ±åˆ
        if df_list:
            self.consolidated_df = pd.concat(df_list, ignore_index=True)
            print(f"\nâœ… Consolidated {len(df_list)} files into {len(self.consolidated_df)} rows")
        else:
            raise ValueError("çµ±åˆå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        
        return self.consolidated_df
    
    def save_consolidated_csv(self, output_filename='consolidated_skill_data.csv'):
        """
        çµ±åˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        
        Args:
            output_filename (str): å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
            
        Returns:
            str: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        if self.consolidated_df is None:
            raise ValueError("çµ±åˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«consolidate_csv_files()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
            print(f"ğŸ“‚ Created output directory: {self.output_dir}")
        
        # å‡ºåŠ›ãƒ‘ã‚¹
        output_path = os.path.join(self.output_dir, output_filename)
        
        # ã‚«ãƒ©ãƒ ã®é †åºã‚’æŒ‡å®š
        column_order = [
            'ã‚«ãƒ†ã‚´ãƒªãƒ¼',
            'ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼',
            'ã‚¹ã‚­ãƒ«é …ç›®',
            'ãƒ­ãƒ¼ãƒ«',
            'å°‚é–€æ€§',
            'ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«',
            'ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«_æ•°å€¤',
            'ãƒ•ã‚¡ã‚¤ãƒ«å'
        ]
        
        # ã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèªã¨ä¸¦ã³æ›¿ãˆ
        existing_columns = [col for col in column_order if col in self.consolidated_df.columns]
        self.consolidated_df = self.consolidated_df[existing_columns]
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        self.consolidated_df.to_csv(output_path, index=False, encoding='utf-8')
        
        print(f"ğŸ’¾ Saved consolidated CSV: {output_path}")
        
        return output_path
    
    def get_summary(self):
        """
        çµ±åˆãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦æƒ…å ±ã‚’å–å¾—
        
        Returns:
            dict: çµ±åˆãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦
        """
        if self.consolidated_df is None:
            return None
        
        return {
            'total_rows': len(self.consolidated_df),
            'total_columns': len(self.consolidated_df.columns),
            'columns': list(self.consolidated_df.columns),
            'unique_files': self.consolidated_df['ãƒ•ã‚¡ã‚¤ãƒ«å'].nunique() if 'ãƒ•ã‚¡ã‚¤ãƒ«å' in self.consolidated_df.columns else 0,
            'unique_roles': self.consolidated_df['ãƒ­ãƒ¼ãƒ«'].nunique() if 'ãƒ­ãƒ¼ãƒ«' in self.consolidated_df.columns else 0,
            'unique_specialities': self.consolidated_df['å°‚é–€æ€§'].nunique() if 'å°‚é–€æ€§' in self.consolidated_df.columns else 0,
            'unique_skills': self.consolidated_df['ã‚¹ã‚­ãƒ«é …ç›®'].nunique() if 'ã‚¹ã‚­ãƒ«é …ç›®' in self.consolidated_df.columns else 0,
        }
    
    def validate_consolidated_data(self):
        """
        çµ±åˆãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
        
        Returns:
            bool: æ¤œè¨¼çµæœï¼ˆTrue: æ­£å¸¸, False: ç•°å¸¸ï¼‰
        """
        if self.consolidated_df is None:
            print("âŒ Error: No consolidated data to validate")
            return False
        
        try:
            # å¿…é ˆã‚«ãƒ©ãƒ ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            required_columns = [
                'ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'ã‚¹ã‚­ãƒ«é …ç›®',
                'ãƒ­ãƒ¼ãƒ«', 'å°‚é–€æ€§', 'ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«', 'ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«_æ•°å€¤', 'ãƒ•ã‚¡ã‚¤ãƒ«å'
            ]
            
            missing_columns = [col for col in required_columns if col not in self.consolidated_df.columns]
            if missing_columns:
                print(f"âŒ Error: Missing columns: {missing_columns}")
                return False
            
            # NULLå€¤ã®ãƒã‚§ãƒƒã‚¯
            if self.consolidated_df.isnull().any().any():
                null_counts = self.consolidated_df.isnull().sum()
                null_columns = null_counts[null_counts > 0]
                print(f"âš ï¸  Warning: Found NULL values:")
                for col, count in null_columns.items():
                    print(f"   - {col}: {count} nulls")
                return False
            
            # ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«ã®å€¤ãƒã‚§ãƒƒã‚¯
            valid_levels = {'a', 'b', 'c', 'd', 'z'}
            if 'ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«' in self.consolidated_df.columns:
                invalid_levels = set(self.consolidated_df['ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«'].unique()) - valid_levels
                if invalid_levels:
                    print(f"âŒ Error: Invalid skill level values found: {invalid_levels}")
                    return False
            
            print("âœ… Data validation passed!")
            return True
            
        except Exception as e:
            print(f"âŒ Validation error: {str(e)}")
            return False


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        print("="*60)
        print("ğŸ”— Skill Data Consolidator")
        print("="*60)
        print("Starting consolidation process...\n")
        
        # ã‚³ãƒ³ã‚½ãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼ã®åˆæœŸåŒ–
        consolidator = SkillConsolidator(input_dir='output', output_dir='output2')
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆ
        print("ğŸ”„ Consolidating CSV files...")
        df = consolidator.consolidate_csv_files()
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
        print("\nğŸ” Data Validation:")
        consolidator.validate_consolidated_data()
        
        # çµ±åˆCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        print("\nğŸ’¾ Saving consolidated CSV...")
        output_path = consolidator.save_consolidated_csv('consolidated_skill_data.csv')
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        summary = consolidator.get_summary()
        print("\nğŸ“Š Consolidation Summary:")
        print(f"   - Total rows: {summary['total_rows']}")
        print(f"   - Total columns: {summary['total_columns']}")
        print(f"   - Source files: {summary['unique_files']}")
        print(f"   - Unique roles: {summary['unique_roles']}")
        print(f"   - Unique specialities: {summary['unique_specialities']}")
        print(f"   - Unique skills: {summary['unique_skills']}")
        print(f"   - File size: {os.path.getsize(output_path):,} bytes")
        
        print("\n" + "="*60)
        print("âœ¨ Consolidation completed successfully!")
        print("="*60)
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
        print("\nğŸ“„ Sample data (first 5 rows):")
        print(df.head())
        
    except FileNotFoundError as e:
        print(f"\nâŒ Error: {e}")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        raise


if __name__ == "__main__":
    main()
