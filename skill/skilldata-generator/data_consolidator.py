"""
ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¾ã¨ã‚ã‚‹
"""

import os
import pandas as pd
from typing import List, Optional
from pathlib import Path


class DataConsolidator:
    """
    ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã™ã‚‹ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, output_dir: str = "output"):
        """
        ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–
        
        Args:
            output_dir (str): å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        """
        self.output_dir = output_dir
        self.samples_dir = os.path.join(output_dir, "samples")
    
    def consolidate_all_data(self) -> Optional[str]:
        """
        å…¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦1ã¤ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            
        Returns:
            Optional[str]: çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
        """
        try:
            consolidated_data = []
            file_info = []
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            sample_files = self._get_sample_files()
            for file_path in sample_files:
                source_name = self._extract_source_name(file_path)
                sample_df = self._load_csv_with_source(file_path, source_name)
                
                if sample_df is not None:
                    consolidated_data.append(sample_df)
                    file_info.append({
                        'source': source_name,
                        'file_name': os.path.basename(file_path),
                        'rows': len(sample_df)
                    })
            
            if not consolidated_data:
                print("âŒ No data files found to consolidate")
                return None
            
            # ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
            consolidated_df = pd.concat(consolidated_data, ignore_index=True)
            
            # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
            consolidated_path = os.path.join(self.output_dir, "consolidated_skill_data.csv")
            consolidated_df.to_csv(consolidated_path, index=False, encoding='utf-8')
            
            # çµ±åˆæƒ…å ±ã®è¡¨ç¤º
            self._display_consolidation_summary(file_info, consolidated_df, consolidated_path)
            
            return consolidated_path
            
        except Exception as e:
            print(f"âŒ Error during data consolidation: {str(e)}")
            return None
    
    def _get_sample_files(self) -> List[str]:
        """
        ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ä¸€è¦§ã‚’å–å¾—
        
        Returns:
            List[str]: ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        sample_files = []
        
        if not os.path.exists(self.samples_dir):
            return sample_files
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã€ç•ªå·é †ã«ã‚½ãƒ¼ãƒˆ
        for file_name in sorted(os.listdir(self.samples_dir)):
            if file_name.endswith('.csv'):
                file_path = os.path.join(self.samples_dir, file_name)
                sample_files.append(file_path)
        
        return sample_files
    
    def _load_csv_with_source(self, file_path: str, source_name: str) -> Optional[pd.DataFrame]:
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¿½åŠ 
        
        Args:
            file_path (str): CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            source_name (str): ã‚½ãƒ¼ã‚¹å
            
        Returns:
            Optional[pd.DataFrame]: èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
        """
        try:
            df = pd.read_csv(file_path, encoding='utf-8')
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æœ€å¾Œã®åˆ—ã¨ã—ã¦è¿½åŠ ï¼ˆæ‹¡å¼µå­é™¤å»ï¼‰
            file_name = os.path.basename(file_path)
            file_name_without_ext = os.path.splitext(file_name)[0]
            df['ãƒ•ã‚¡ã‚¤ãƒ«å'] = file_name_without_ext
            
            return df
            
        except Exception as e:
            print(f"âš ï¸  Warning: Failed to load {file_path}: {str(e)}")
            return None
    
    def _extract_source_name(self, file_path: str) -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ã‚½ãƒ¼ã‚¹åã‚’æŠ½å‡º
        
        Args:
            file_path (str): ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            str: ã‚½ãƒ¼ã‚¹å
        """
        file_name = os.path.basename(file_path)
        
        # sample_XXX_strategy.csv ã®å½¢å¼ã‹ã‚‰strategyéƒ¨åˆ†ã‚’æŠ½å‡º
        if file_name.startswith('sample_') and file_name.endswith('.csv'):
            parts = file_name[:-4].split('_')  # .csvã‚’é™¤å»ã—ã¦åˆ†å‰²
            if len(parts) >= 3:
                # sample_001_engineer_focused -> engineer_focused
                strategy = '_'.join(parts[2:])
                return f"sample_{strategy}"
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return file_name[:-4] if file_name.endswith('.csv') else file_name
    
    def _display_consolidation_summary(self, file_info: List[dict], 
                                     consolidated_df: pd.DataFrame, 
                                     output_path: str):
        """
        çµ±åˆçµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        
        Args:
            file_info (List[dict]): ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®ãƒªã‚¹ãƒˆ
            consolidated_df (pd.DataFrame): çµ±åˆãƒ‡ãƒ¼ã‚¿
            output_path (str): å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        print("\n" + "="*60)
        print("ğŸ“Š Data Consolidation Summary")
        print("="*60)
        
        print(f"ğŸ“ Output file: {output_path}")
        print(f"ğŸ“ˆ Total consolidated rows: {len(consolidated_df):,}")
        print(f"ğŸ“„ Source files included: {len(file_info)}")
        
        print("\nğŸ“‹ File breakdown:")
        for info in file_info:
            print(f"   â€¢ {info['file_name']}: {info['rows']:,} rows ({info['source']})")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥ã®é›†è¨ˆ
        source_counts = consolidated_df['ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹'].value_counts()
        print("\nğŸ“Š Data distribution by source:")
        for source, count in source_counts.items():
            percentage = (count / len(consolidated_df)) * 100
            print(f"   â€¢ {source}: {count:,} rows ({percentage:.1f}%)")
        
        print("="*60)
    
    def validate_consolidated_data(self, consolidated_path: str) -> bool:
        """
        çµ±åˆãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’æ¤œè¨¼
        
        Args:
            consolidated_path (str): çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            bool: æ¤œè¨¼çµæœ
        """
        try:
            df = pd.read_csv(consolidated_path, encoding='utf-8')
            
            # å¿…é ˆåˆ—ã®å­˜åœ¨ç¢ºèª
            required_columns = [
                'ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹', 'ãƒ•ã‚¡ã‚¤ãƒ«å', 'ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼', 
                'ã‚¹ã‚­ãƒ«é …ç›®', 'ãƒ­ãƒ¼ãƒ«', 'å°‚é–€æ€§', 'ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«', 'ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«_æ•°å€¤'
            ]
            
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                print(f"âŒ Missing columns: {missing_columns}")
                return False
            
            # NULLå€¤ã®ãƒã‚§ãƒƒã‚¯
            if df[required_columns].isnull().any().any():
                print("âŒ Found NULL values in required columns")
                return False
            
            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ä¸€æ„æ€§ãƒã‚§ãƒƒã‚¯
            unique_sources = df['ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹'].nunique()
            if unique_sources < 1:  # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒæœ€ä½1ã¤
                print("âŒ Insufficient data sources")
                return False
            
            print("âœ… Consolidated data validation passed!")
            return True
            
        except Exception as e:
            print(f"âŒ Error validating consolidated data: {str(e)}")
            return False